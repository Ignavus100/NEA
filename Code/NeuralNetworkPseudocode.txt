CLASS Network
    ARR layers
    MATRIX y
    PROCEDURE instantiate(self, inp_size, out_size, hidden_amount, hidden_size)
        y = fetch from traning data file for the batch in question
        layers.APPEND(Layer(0, inp_size))
        FOR i in hidden_amount
            IF i = 0
                THEN layers.APPEND(Layer(inp_size, hidden_size))
            ELSE
                layers.APPEND(Layer(hidden_size, hidden_size))
        layers.APPEND(Layer(hidden_size, out_size))
    
    PROCEDURE forward(self)
        self.layers[-1].forward()
        return self.layers[-1], self.layers[-1].cost(y)

    PROCEDURE backwards(self)
        self.layers[0].backwards()

CLASS Layer
    MATRIX weights
    MATRIX bias
    MATRIX activation
    FLOAT cost
    MATRIX y
    MATRIX prev_layer
    PROCEDURE instantiate(self, prev_neuron_amount, curr_neuron_amount)
        weights = random numbers in dimensions curr_neuron_amount x prev_neuron_amount
        prev_layer = fetch from prev neuron if this is not input layer
        activation = Null's in matrix dimensions curr_neuron_amount x 1
        bias = random numbers in matrix dimensions prev_neuron_amount x 1
        IF layer = output_layer
            THEN y = fetch from network class 
        ELSE
            y = zeros in dimensions of activation


    PROCEDURE forward(self)
        WHILE activation[0] is Null
            IF prev_layer.activation[0] is not Null
                THEN activation <- SIG(weights * prev_layer.activation + bias)
            ELSE
                IF prev_layer != input_layer
                    THEN prev_layer.forward()
                ELSE
                    return 0
                ENDIF     
            ENDIF

    PROCEDURE cost_calculation(self, expected_output)
        for i -> len(expected_output)
            cost += (activation[i] - expected_output[i])**2

    PROCEDURE cost_calculation_dir(self, expected_output)
        FLOAT temp
        for i -> len(expected_output)
            temp += (activation[i] - expected_output[i])*2
        return temp

    PROCEDURE backwards(self)
        Network.forward()
        cost = cost_calculation(y)

        #CHANGING WEIGHTS 
        FOR i in weights(x_direction)
            FOR j in weights(y_direction)
                weights[i][j] = weights[i][j] - (cost)/(   (prev_layer.activation[j])   *   (SIG_DIR(weights * prev_layer.activation + bias)[j])   *   (cost_calculation_dir(y[j]))   )
        
        #CREATE NEW Y FOR NEXT LAYER
        FOR i -> len(prev_layer.activation)
            temp_y = 0
            temp_weights = 0
            FOR j -> len(activation)
                temp_weights += weights[i][j]
            FOR j -> len(activation)
                temp_y += (   (temp_weights)   *   (SIG_DIR(weights * prev_layer.activation + bias)[j])   *   (cost_calculation_dir(y[j])))
            prev_layer.y.append(temp_y)

        #CHANGING BIAS 
        FOR j in len(bias)
            weights[i][j] = weights[i][j] - (cost)/(   (1)   *   (SIG_DIR(weights * prev_layer.activation + bias)[j])   *   (cost_calculation_dir(y[j]))   )

        if not input_layer
            THEN prev_layer.backwards()

PROCEDURE SIG(arr)
    FOR i -> len(arr):
        arr[i] = 1/(1+exp(-arr[i]))

PROCEDURE SIG_DIR(val)
    return SIG(val)(1-SIG(val))